# -*- coding: utf-8 -*-
"""Birds_Detection.Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D6vpcVP_rrX1RuGhuDh3rfxh5TvOC__e
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import random
from PIL import Image
import seaborn as sns
import tensorflow as tf

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %cd /content/drive/MyDrive/datasets

bird_data = pd.read_excel("/content/drive/MyDrive/datasets/Birds_detection/Birds.xlsx")

"""Exploring the data"""

bird_data.head()

bird_data.info()

bird_data.describe(include = 'all')

#checking for missing  values
bird_data.isnull().sum()

# count the occurrences of each unique value in the "labels" column
bird_data.groupby("labels")["labels"].value_counts()

data_path = "/content/drive/MyDrive/datasets/Birds_detection"

#exploring the images data

num_image_show = 5
for _ in range(num_image_show):
    # Choose a random species directory
    species_dirs = os.listdir(os.path.join(data_path, 'train'))
    random_species = random.choice(species_dirs)

    # Choose a random image from that species
    species_path = os.path.join(data_path, 'train', random_species)
    random_image = random.choice(os.listdir(species_path))

    # Load and display the image
    image_path = os.path.join(species_path, random_image)
    image = Image.open(image_path)

    plt.figure()
    plt.imshow(image)
    plt.title(f'Species: {random_species}')
    plt.axis('off')

plt.show()

#loading the data
train_data_path = "/content/drive/MyDrive/datasets/Birds_detection/train"
valid_data_path = "/content/drive/MyDrive/datasets/Birds_detection/valid"
test_data_path = "/content/drive/MyDrive/datasets/Birds_detection/test"

# Image size for resizing
image_size = (224, 224)
batch_size = 32                  # Batch size for training
num_classes = len(bird_data["labels"].unique())   # Number of classes in the dataset

# Importing the ImageDataGenerator class from TensorFlow's Keras module
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Configuration of ImageDataGenerator for training data augmentation
train_datagen = ImageDataGenerator(
    rescale=1.0/255.0,  # Normalize pixel values to [0, 1]
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Configuration of ImageDataGenerator for validation data and testing data
valid_datagen = ImageDataGenerator(rescale=1.0/255.0)
test_datagen = ImageDataGenerator(rescale=1.0/255.0)

# Creating data generators for training, validation, and test datasets
train_generator = train_datagen.flow_from_directory(
    train_data_path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical'
)

valid_generator = valid_datagen.flow_from_directory(
    valid_data_path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_data_path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical'
)

"""# Building the model"""

import tensorflow as tf

# 1. Load the base InceptionV3 model
base_model = tf.keras.applications.InceptionV3(include_top=False)
# 2. Freeze the base model
base_model.trainable = False

# 3. Define data augmentation
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.RandomFlip("horizontal"),
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
])

# 4. Create the custom model
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(224, 224, 3), name="input-layer"),
    data_augmentation,
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(name="global_average_pooling_layer"),
    tf.keras.layers.Dense(30, activation="softmax", name="output-layer")
])

# 5. Compile the model
model.compile(loss="categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
                metrics=["accuracy"])

# 6. Add Early Stopping
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',  # Monitor validation loss
    patience=5,           # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity
)

model.summary()

history = model.fit(
    train_generator,
    epochs=20,
    steps_per_epoch=len(train_generator),
    validation_data=valid_generator,
    validation_steps=len(valid_generator),
    callbacks=[early_stopping]
)

# Save the trained model to a file
model.save('bird_detection_model.h5')

import pickle
model_filename = 'model.pkl'
with open(model_filename, 'wb') as file:
    pickle.dump(model, file)

# Extract Metrics from History
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# Plot training & validation loss
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(train_loss, label='Training Loss', color='blue')
plt.plot(val_loss, label='Validation Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training & Validation Loss')

# Plot training & validation accuracy
plt.subplot(1, 2, 2)
plt.plot(train_acc, label='Training Accuracy', color='blue')
plt.plot(val_acc, label='Validation Accuracy', color='red')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training & Validation Accuracy')

plt.tight_layout()
plt.show()

from tensorflow.keras.models import load_model

# Load the saved model
loaded_model = load_model('bird_detection_model.h5')

# Use the loaded model for predictions
# predictions = loaded_model.predict(input_data)

# Evaluate the model on the test dataset
test_loss, test_accuracy = loaded_model.evaluate(test_generator)
print(f"Test accuracy: {test_accuracy}")

#the backend to be deployed
from flask import Flask, request, jsonify
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.inception_v3 import preprocess_input
import numpy as np

app = Flask(__name__)

# Load the saved model
loaded_model = load_model('bird_detection_model.h5')

# Define a route for prediction
@app.route('/predict', methods=['POST'])
def predict():
    try:
        # Get the image file from the request
        image_file = request.files['image']

        # Load and preprocess the image
        img = image.load_img(image_file, target_size=(299, 299))  # Assuming InceptionV3 input size
        img_array = image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        img_array = preprocess_input(img_array)

        # Perform prediction using the loaded model
        predictions = loaded_model.predict(img_array)



        # Return the predictions as JSON
        return jsonify({'predictions': predictions.tolist()})

    except Exception as e:
        return jsonify({'error': str(e)})

if __name__ == '__main__':
    app.run(debug=True)